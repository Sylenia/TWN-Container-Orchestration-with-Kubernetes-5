# Deploying a Microservices Application in Kubernetes with Production & Security Best Practices

## Project Overview
This demo project involves deploying a microservices-based online shop application on a Kubernetes cluster with production and security best practices. The application consists of multiple services communicating via gRPC, and we leverage Linodeâ€™s Kubernetes Engine (LKE) to host the application. The deployment demonstrates secure and efficient management of services, resource optimization, and scalability.

---

## Technologies Used
- **Kubernetes**
- **Redis**
- **Linux**
- **Linode LKE**

---

## Key Concepts

### Microservices Architecture
- Microservices are small, independently deployable services that form the building blocks of modern applications.
- Each service in the online shop application serves a specific function, such as checkout, recommendation, or email notifications.

### Kubernetes in Production
- Kubernetes orchestrates deployment, scaling, and management of containerized applications.
- Production-grade deployments include liveness and readiness probes, resource limits, and secure configurations.

### Security Best Practices
- Use Kubernetes Secrets for sensitive data.
- Apply Role-Based Access Control (RBAC) to restrict access.
- Configure network policies to limit communication between pods.

---

## Steps to Deploy the Application

### Step 1: Set Up Linode Kubernetes Engine (LKE)
1. **Create a Managed Cluster**:
   - Log in to your Linode account and create an LKE cluster.
   - Choose a node size and count suitable for your application.
   - Save the Kubernetes configuration file (`kubeconfig`) to your local machine.
2. **Access the Cluster**:
   - Export the Kubeconfig file:
     ```bash
     export KUBECONFIG=/path/to/your-kubeconfig.yaml
     ```
   - Test the connection:
     ```bash
     kubectl get nodes
     ```

---

### Step 2: Configure Access to Linode Server
1. **Secure Access**:
   - Use SSH keys for authentication.
   - Generate an SSH key pair if not already done:
     ```bash
     ssh-keygen -t rsa -b 4096 -C "your_email@example.com"
     ```
   - Add the public key to your Linode server.
2. **Connect to the Linode Server**:
   ```bash
   ssh root@<LINODE_IP>
   ```

---

### Step 3: Prepare the Kubernetes Cluster
1. **Install Minikube for Local Testing**:
   ```bash
   minikube start
   ```
2. **Apply RBAC Policies**:
   - Create a `ClusterRole` and `ClusterRoleBinding` for admin access.

---

### Step 4: Deploy the Microservices

#### Sample Deployment (Email Service)
1. **Create Deployment YAML**:
   ```yaml
   apiVersion: apps/v1
   kind: Deployment
   metadata:
     name: emailservice
   spec:
     replicas: 2
     selector:
       matchLabels:
         app: emailservice
     template:
       metadata:
         labels:
           app: emailservice
       spec:
         containers:
         - name: service
           image: gcr.io/google-samples/microservices-demo/emailservice:v0.8.0
           ports:
           - containerPort: 8080
           livenessProbe:
             grpc:
               port: 8080
             periodSeconds: 5
           readinessProbe:
             grpc:
               port: 8080
             periodSeconds: 5
           resources:
             requests:
               cpu: 100m
               memory: 64Mi
             limits:
               cpu: 200m
               memory: 128Mi
   ```
2. **Apply the Deployment**:
   ```bash
   kubectl apply -f emailservice-deployment.yaml
   ```

#### Create Associated Service
1. **Service YAML**:
   ```yaml
   apiVersion: v1
   kind: Service
   metadata:
     name: emailservice
   spec:
       type: ClusterIP
       selector:
         app: emailservice
       ports:
       - protocol: TCP
         port: 5000
         targetPort: 8080
   ```
2. **Apply the Service**:
   ```bash
   kubectl apply -f emailservice-service.yaml
   ```

---

### Step 5: Configure Redis for Cart Service
1. **Redis Deployment YAML**:
   ```yaml
   apiVersion: apps/v1
   kind: Deployment
   metadata:
     name: redis-cart
   spec:
     replicas: 2
     selector:
       matchLabels:
         app: redis-cart
     template:
       metadata:
         labels:
           app: redis-cart
       spec:
         containers:
         - name: redis
           image: redis:alpine
           ports:
           - containerPort: 6379
           volumeMounts:
           - name: redis-data
             mountPath: /data
         volumes:
         - name: redis-data
           emptyDir: {}
   ```
2. **Apply the Deployment**:
   ```bash
   kubectl apply -f redis-deployment.yaml
   ```

---

### Step 6: Enable Load Balancer for Frontend
1. **Frontend Service YAML**:
   ```yaml
   apiVersion: v1
   kind: Service
   metadata:
     name: frontend
   spec:
     type: LoadBalancer
     selector:
       app: frontend
     ports:
     - protocol: TCP
       port: 80
       targetPort: 8080
   ```
2. **Apply the Service**:
   ```bash
   kubectl apply -f frontend-service.yaml
   ```
3. **Access the Application**:
   - Get the external IP:
     ```bash
     kubectl get services
     ```
   - Open the IP in a browser.

---

### Step 7: Secure Application with Secrets
1. **Create Kubernetes Secrets**:
   ```bash
   kubectl create secret generic app-secrets --from-literal=DB_PASSWORD=mysecurepassword
   ```
2. **Use Secrets in Deployment**:
   ```yaml
   env:
   - name: DB_PASSWORD
     valueFrom:
       secretKeyRef:
         name: app-secrets
         key: DB_PASSWORD
   ```

---

## Best Practices
1. **Monitor and Log**:
   - Use tools like Prometheus and Grafana.
   - Enable centralized logging with Elasticsearch or Fluentd.
2. **Optimize Resources**:
   - Set appropriate resource limits and requests for each service.
3. **Scale Effectively**:
   - Use Horizontal Pod Autoscalers.
4. **Implement Security**:
   - Use Secrets for sensitive data.
   - Apply network policies for service-to-service communication.

---

## Wrapping It Up
Deploying a microservices application in Kubernetes isnâ€™t just about throwing YAML files at the cluster. Itâ€™s about balancing performance, scalability, and security. With production-ready configurations, services that talk to each other seamlessly, and tools like Redis and Linode LKE, youâ€™re not just running an applicationâ€”youâ€™re crafting an ecosystem thatâ€™s built to thrive. Letâ€™s keep shipping great software! ðŸš€ðŸš€ðŸš€
